{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnderstandingLossFunctions_exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EiffL/AstroHackWeek2019/blob/master/day4_ml/UnderstandingLossFunctions_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byr6_uC0Viyx",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2019 Francois Lanusse.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq5qqz-kVoAq",
        "colab_type": "text"
      },
      "source": [
        "## Understanding Regression Loss Functions And Introduction to TensorFlow Probability\n",
        "\n",
        "Author: [@EiffL](https://github.com/EiffL) (Francois Lanusse)\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this short notebook, we will try to solve a simple regression problem where \n",
        "the error distribution is unknown, and in particular heteroscedastic and asymetric. This will teach us about what information is recovered by different \n",
        "usual regression loss functions like mean absolute error and mean square error losses. And finally, we will see that using [TensorFlow Probability](https://www.tensorflow.org/probability) (TFP) we can model \n",
        "a full posterior probability and extract any summary we want from it.\n",
        "\n",
        "### Learning goals\n",
        "\n",
        "In this notebook, we will learn:\n",
        "*   The probabilistic interpretation of common regression loss functions\n",
        "*   How to use TFP for modelling distributions in Keras\n",
        "*   How to build and use a Mixture Density Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7dtJZA4VgUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Imports { display-mode: \"form\" }\n",
        "%pylab inline\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Activate TF2 behavior:\n",
        "from tensorflow.python import tf2\n",
        "if not tf2.enabled():\n",
        "  import tensorflow.compat.v2 as tf\n",
        "  tf.enable_v2_behavior()\n",
        "  assert tf2.enabled()\n",
        "\n",
        "#Checking for GPU access\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BawVZXB_kO5a",
        "colab_type": "text"
      },
      "source": [
        "If the GPU fails to load, activate the GPU hardware acceleration in `Runtime > Change Runtime Type`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlfcYKnZWjEQ",
        "colab_type": "text"
      },
      "source": [
        "## Generating some data\n",
        "\n",
        "We begin by generating some data sampled from a distribution $p(y | x)$ constructed from a log normal distribution with \n",
        "\n",
        "log normal distribution with position and sigma "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHaUm5B2Wf1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import lognorm\n",
        "\n",
        "x = linspace(0.1,1.5)\n",
        "def model(x):\n",
        "  return lognorm.rvs(s=x, loc=2*x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-A6JXfDWsI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scatter(x, model(x))\n",
        "xlabel('x')\n",
        "ylabel('y')\n",
        "ylim(0,12);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhcAQvnZltt9",
        "colab_type": "text"
      },
      "source": [
        "The task will be:\n",
        "```\n",
        "Given this data, solve a regression problem and estimate y given x.\n",
        "```\n",
        "As we will see in this notebook, this is not well defined at all ;-) There is no\n",
        "single answer, the problem is degenerate, for a given $x$ there is actually a full distribution of possible $y$. What is regression in this context, do we mean the mean, the mode, or the median of $p(y | x)$, and if so, how do we estimate them?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rEmK2VtW1QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scatter(x, model(x))\n",
        "plot(x, lognorm.mean(x, loc=2*x), label='p(y|x) mean')\n",
        "plot(x, lognorm.median(x, loc=2*x), label='p(y|x) median')\n",
        "plot(x, exp(- x**2) + 2*x, label='p(y|x) mode')\n",
        "ylim(0,12)\n",
        "legend()\n",
        "xlabel('x')\n",
        "ylabel('y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAl1PcTcmu2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building training and testing sets \n",
        "x_train = np.random.uniform(0.1,1.5,100*1000).reshape(1000,100)\n",
        "y_train = model(x_train)\n",
        "\n",
        "x_test = np.random.uniform(0.1,1.5,100*1000).reshape(1000,100)\n",
        "y_test = model(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP24f8MLnOKc",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1: Regression with l1 and l2 losses\n",
        "\n",
        "In this first exercise, let's attempt a standard regression. We begin with the \n",
        "usual mean squared error, a.k.a. l2 loss $\\mathcal{L} = \\parallel y - f_\\varphi(x) \\parallel_2^2$.\n",
        "Here is a simple Keras model using an l2 loss for the regressoin:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdfz2oXYagHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "\n",
        "learning_rate=1e-3\n",
        "decay=1e-6\n",
        "\n",
        "l2_model = keras.Sequential([\n",
        "    keras.layers.Dense(units=128, activation='relu', input_shape=(1,)),\n",
        "    keras.layers.Dense(units=128, activation='tanh'),\n",
        "    keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "l2_model.compile(loss='mean_squared_error', optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyu_RPrioZny",
        "colab_type": "code",
        "outputId": "037f960b-0310-4119-d40a-ea5731061ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "# Train the model\n",
        "l2_model.fit(x_train.flatten(), y_train.flatten(), batch_size=256, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "100000/100000 [==============================] - 1s 11us/sample - loss: 8.9712\n",
            "Epoch 2/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6742\n",
            "Epoch 3/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6647\n",
            "Epoch 4/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6647\n",
            "Epoch 5/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6548\n",
            "Epoch 6/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6539\n",
            "Epoch 7/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6626\n",
            "Epoch 8/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6541\n",
            "Epoch 9/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6577\n",
            "Epoch 10/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6545\n",
            "Epoch 11/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6578\n",
            "Epoch 12/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6499\n",
            "Epoch 13/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6496\n",
            "Epoch 14/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6512\n",
            "Epoch 15/20\n",
            "100000/100000 [==============================] - 1s 11us/sample - loss: 8.6542\n",
            "Epoch 16/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6465\n",
            "Epoch 17/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6454\n",
            "Epoch 18/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6483\n",
            "Epoch 19/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6568\n",
            "Epoch 20/20\n",
            "100000/100000 [==============================] - 1s 10us/sample - loss: 8.6451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f11c8dd4438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4JpVgcMapTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the regression model on the testing set\n",
        "preds = l2_model.predict(x_test.flatten())\n",
        "preds = preds.reshape(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-khUAlla9bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scatter(x_test[0], preds[0])\n",
        "scatter(x_test[0], y_test[0])\n",
        "xlabel('x')\n",
        "ylabel('y')\n",
        "xlim(0.1,1.5)\n",
        "ylim(0,12)\n",
        "plot(x, lognorm.mean(x, loc=2*x), label='p(y|x) mean')\n",
        "plot(x, lognorm.median(x, loc=2*x), label='p(y|x) median')\n",
        "plot(x, exp(- x**2) + 2*x, label='p(y|x) mode')\n",
        "legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_npA-iuqdhH",
        "colab_type": "text"
      },
      "source": [
        "Interesting! Turns out that the mean squared error actually leads to estimating  the mean of the distribution $p(y | x)$. As we see here, the mean is not always a great summary of the distribution, it is heavily influenced by the tails of the distribution and can become very different from the mode.\n",
        "\n",
        "**Homework**: Show analytically that MSE regression correponds to estimating the posterior mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vI9iG-OrfXW",
        "colab_type": "text"
      },
      "source": [
        "Now, let's do the same thing but with the mean absolute error, a.k.a. l1 loss $\\mathcal{L} = |y - f(x)|$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ_45xLxa_W0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "\n",
        "learning_rate=1e-3\n",
        "decay=1e-6\n",
        "\n",
        "l1_model = keras.Sequential([\n",
        "    keras.layers.Dense(units=128, activation='relu', input_shape=(1,)),\n",
        "    keras.layers.Dense(units=128, activation='tanh'),\n",
        "    keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "# !!! Exercise compile model to mean_absolute_error as the loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDXSo5cibRG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1_model.fit(x_train.flatten(), y_train.flatten(), batch_size=256, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmX-95-ksNmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the regression model on the testing set\n",
        "preds = l1_model.predict(x_test.flatten())\n",
        "preds = preds.reshape(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfuZ6zDLsZqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scatter(x_test[0], preds[0])\n",
        "scatter(x_test[0], y_test[0])\n",
        "xlabel('x')\n",
        "ylabel('y')\n",
        "xlim(0.1,1.5)\n",
        "ylim(0,12)\n",
        "plot(x, lognorm.mean(x, loc=2*x), label='p(y|x) mean')\n",
        "plot(x, lognorm.median(x, loc=2*x), label='p(y|x) median')\n",
        "plot(x, exp(- x**2) + 2*x, label='p(y|x) mode')\n",
        "legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUc4eAJJspfr",
        "colab_type": "text"
      },
      "source": [
        "Bang on on the median! So the lesson here is that minimizing the mean absolute error leads to estimating the median of the distribution $p(y | x)$. This can be more robust than l2 regression, the median is more robust to outliers and ends up being much closer to the mode of the distribution.\n",
        "\n",
        "\n",
        "**Homework**: Show analytically that MAE regression correponds to estimating the posterior median."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVRr3De4sh5i",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2: Switch to Probabilistic Modelling\n",
        "\n",
        "What we have seen in the previous exercise is that different choices of regression loss functions lead to different point estimate summaries of a full distribution $p(y|x)$. But in a complex case like this one, the mode of the distribution is different from both mean and median, so how to get to it?\n",
        "\n",
        "Also, we are starting to see that summaries do not give a full picture of the problem. Let's try to actually learn the full posterior with TensorFlow Probability.\n",
        "\n",
        "The  first thing we need is a parametric model for the distribution $q_{\\varphi}(y|x)$ that the model will be predicting. A simple approach to build a flexible\n",
        "distribution is to use a Gaussian Mixture Density. With TensorFlow Probability this is easily done by adding a `tfp.layers.MixtureNormal` as the last layer of the model. Check out the documentation here: https://www.tensorflow.org/probability/api_docs/python/tfp/layers/MixtureNormal\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd14A5m4bu5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "learning_rate=1e-3\n",
        "decay=1e-6\n",
        "\n",
        "# Number of components in the Gaussian Mixture\n",
        "num_components = 16\n",
        "# Shape of the distribution\n",
        "event_shape = [1]\n",
        "# Utility function to compute how many parameters this distribution requires\n",
        "params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape)\n",
        "\n",
        "gmm_model = keras.Sequential([\n",
        "    keras.layers.Dense(units=128, activation='relu', input_shape=(1,)),\n",
        "    keras.layers.Dense(units=128, activation='tanh'),\n",
        "    keras.layers.Dense(params_size),\n",
        "    tfp.layers.MixtureNormal(num_components, event_shape)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr1exFC_wQp_",
        "colab_type": "text"
      },
      "source": [
        "An important thing to note is that now the model doesn't just return a number, it returns a full distribution object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK3IfW_hwKgo",
        "colab_type": "code",
        "outputId": "99ac36ef-ae32-4f7b-bc9b-c2f9e59b0957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# We reshape x to match the event size of 1\n",
        "gmm_model(x_test.reshape((-1,1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tfp.distributions.MixtureSameFamily 'sequential_5/mixture_normal_1/MixtureSameFamily/MixtureSameFamily/' batch_shape=[100000] event_shape=[1] dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9V48Vd4w2iZ",
        "colab_type": "text"
      },
      "source": [
        "This distribution object can be used to sample from the model, or evaluate the log probability of the model.\n",
        "\n",
        "Now that we have a model, let's train it. For that we need to define the loss function for Keras, which will be based on the negative log likelihood:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqhdosHswK-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that Keras will call this loss function with two arguments, the data\n",
        "# point y, and the distribution q(y | x) predicted as the output of the model\n",
        "negloglik = lambda y, q: -q.log_prob(y)\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "gmm_model.compile(loss=negloglik, optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWcH74G4cLaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gmm_model.fit(x_train.reshape((-1,1)), y_train.reshape((-1,1)), batch_size=256, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxkOuIjMcL1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the mode by sampling the pdf\n",
        "x_smpl = linspace(0.5,5,50)\n",
        "\n",
        "# We evaluate the log probability of the model at different points, to sample the pdf of\n",
        "# each example\n",
        "preds = np.stack([gmm_model(x_test.reshape((-1,1))).log_prob(x).numpy() for x in x_smpl])\n",
        "mode = x_smpl[preds.argmax(axis=0)]\n",
        "mode = mode.reshape(y_test.shape)\n",
        "\n",
        "scatter(x_test[1], mode[1])\n",
        "scatter(x_test[1], y_test[1])\n",
        "\n",
        "xlim(0.1,1.5)\n",
        "ylim(0,12)\n",
        "\n",
        "plot(x, lognorm.mean(x, loc=2*x), label='mean')\n",
        "plot(x, lognorm.median(x, loc=2*x), label='median')\n",
        "plot(x, exp(- x**2) + 2*x, label='mode')\n",
        "legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2YdW_2SnD1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can also have a look at individual example pdfs\n",
        "plot(x_smpl, exp(preds[:,2]))\n",
        "plot(x_smpl, lognorm.pdf(x_smpl, s=x_test.flatten()[2], loc=2*x_test.flatten()[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tAkR28zGCa",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "As soon as the data contains uncertainties, the solution to a regression problem is not something well defined. As we have seen, classical loss functions\n",
        "allow us to estimate summaries of the full problem posterior, but this has its limits when that posterior is not trivial.\n",
        "\n",
        "Using a distribution as the output of the model allows us model the full problem posterior, which can then be manipulated to locate the mode, or derive confidence intervals! And all we did was to add one TFP layer to the model!"
      ]
    }
  ]
}