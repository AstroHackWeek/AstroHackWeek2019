{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Computation\n",
    "\n",
    "This jupyter notebook accompanies the Bayesian Shenaniganry lecture(s) delivered by Justin Alsing and Stephen Feeney as part of [Astro Hack Week 2019](http://astrohackweek.org/2019/). As part of the lecture(s) you will be asked to complete a number of tasks, some of which will involve direct coding into the notebook; these sections are marked by task. This notebook requires numpy, matplotlib, scipy, [corner](https://corner.readthedocs.io/en/latest/), [pystan](https://pystan.readthedocs.io/en/latest/getting_started.html) and pickle to run (the last two are required solely for the final task). If you don't have these packages fret not: the notebook can be run on [Google Colab](https://colab.research.google.com/github/sfeeney/ahw19/blob/master/ahw_complete.ipynb).\n",
    "\n",
    "We start with imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# make sure everything we need is installed if running on Google Colab\n",
    "def is_colab():\n",
    "    try:\n",
    "        cfg = get_ipython().config\n",
    "        if cfg['IPKernelApp']['kernel_class'] == 'google.colab._kernel.Kernel':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except NameError:\n",
    "        return False\n",
    "if is_colab():\n",
    "    !pip install --quiet numpy matplotlib scipy corner pystan emcee\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as mp\n",
    "import scipy.stats as sps\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Write a Python function simulating a very simple linear process observed at known positions $x$ with Gaussian measurement noise: $\\hat{y} = m x + c + n$, where $n \\sim N(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior settings\n",
    "mu_slope = 1.0\n",
    "sig_slope = 0.1\n",
    "mu_intcpt = 0.0\n",
    "sig_intcpt = 0.05\n",
    "mu_var_noise = 0.02\n",
    "var_var_noise = mu_var_noise ** 2 / 9.0\n",
    "shape_var_noise = mu_var_noise ** 2 / var_var_noise + 2.0\n",
    "scale_var_noise = mu_var_noise * (mu_var_noise ** 2 / var_var_noise + 1.0)\n",
    "\n",
    "# super-simple wrapper\n",
    "def x_grid(n_x, x_min=0.0, x_max=1.0):\n",
    "    \n",
    "    return np.linspace(x_min, x_max, n_x)\n",
    "\n",
    "# draw sample from parameter prior\n",
    "def sample_prior(mu_slope, sig_slope, mu_intcpt, sig_intcpt):\n",
    "    \n",
    "    pars = np.zeros(2)\n",
    "    pars[0] = npr.normal(mu_slope, sig_slope)\n",
    "    pars[1] = npr.normal(mu_intcpt, sig_intcpt)\n",
    "    \n",
    "    return pars\n",
    "\n",
    "# generate noisy data given parameters\n",
    "def sample_data(x, pars, var_noise):\n",
    "    \n",
    "    y = pars[0] * x + pars[1] + npr.normal(0.0, np.sqrt(var_noise), len(x))\n",
    "    \n",
    "    return y\n",
    "\n",
    "# most basic simulator function: noisy measurements at known locations\n",
    "def simulator(x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, var_noise):\n",
    "    \n",
    "    # draw parameters from prior\n",
    "    pars = sample_prior(mu_slope, sig_slope, mu_intcpt, sig_intcpt)\n",
    "    \n",
    "    # generate data\n",
    "    y = sample_data(x, pars, var_noise)\n",
    "    \n",
    "    return [pars, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put together some quick checks that everything's working as planned. First, plot some draws from the prior..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of draws from priors\n",
    "pars = []\n",
    "for i in range(10000):\n",
    "    pars.append(sample_prior(mu_slope, sig_slope, mu_intcpt, sig_intcpt))\n",
    "pars = np.array(pars)\n",
    "par_names = [r'm', r'c']\n",
    "\n",
    "# plot versus expectations\n",
    "print('parameter means & standard deviations')\n",
    "fig, axes = mp.subplots(1, 2, figsize=(12, 5))\n",
    "for i in range(2):\n",
    "    print('*', par_names[i])\n",
    "    print('  reals:  ', np.mean(pars[:, i]), np.std(pars[:, i]))\n",
    "    _ = axes[i].hist(pars[:, i], bins=30, density=True)\n",
    "    par_lims = axes[i].get_xlim()\n",
    "    par_grid = np.linspace(par_lims[0], par_lims[1], 100)\n",
    "    axes[i].set_xlabel('$' + par_names[i] + '$')\n",
    "    axes[i].set_ylabel(r'${\\rm P(}' + par_names[i] + ')$')\n",
    "    if i == 0:\n",
    "        print('  theory: ', mu_slope, sig_slope)\n",
    "        axes[i].plot(par_grid, sps.norm.pdf(par_grid, mu_slope, sig_slope))\n",
    "    elif i == 1:\n",
    "        print('  theory: ', mu_intcpt, sig_intcpt)\n",
    "        axes[i].plot(par_grid, sps.norm.pdf(par_grid, mu_intcpt, sig_intcpt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now some simulated data to check the noise distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a large data realization with fixed noise variance\n",
    "x = x_grid(10000)\n",
    "pars, y = simulator(x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, mu_var_noise)\n",
    "noise = y - (pars[0] * x + pars[1])\n",
    "\n",
    "# plot noise histogram versus expectation\n",
    "fig, axes = mp.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(x, y)\n",
    "axes[0].plot(x, pars[0] * x + pars[1])\n",
    "axes[0].set_xlabel('$x$')\n",
    "axes[0].set_ylabel('$y$')\n",
    "_ = axes[1].hist(noise, bins=30, density=True)\n",
    "par_lims = axes[1].get_xlim()\n",
    "par_grid = np.linspace(par_lims[0], par_lims[1], 100)\n",
    "axes[1].plot(par_grid, sps.norm.pdf(par_grid, 0.0, np.sqrt(mu_var_noise)))\n",
    "axes[1].set_xlabel('$n$')\n",
    "axes[1].set_ylabel(r'${\\rm P(}n)$')\n",
    "print('noise mean & standard deviation')\n",
    "print('  reals:  ', np.mean(noise), np.std(noise))\n",
    "print('  theory: ', 0.0, np.sqrt(mu_var_noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Code up the gridded posterior. Let's split this into two parts. First, write three functions evaluating the log-likelihood, log-prior and log-posterior for a set of sampled parameters. These will be very useful in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_like(pars, x, y, var_noise):\n",
    "    \n",
    "    delta = y - (pars[0] * x + pars[1])\n",
    "    cinv = np.diag(np.ones(len(x)) / var_noise)\n",
    "    ln_like_y = -0.5 * np.dot(np.dot(delta, cinv), delta)\n",
    "        \n",
    "    return ln_like_y\n",
    "\n",
    "def ln_prior(pars, mu_slope, sig_slope, mu_intcpt, sig_intcpt):\n",
    "    return -0.5 * (((pars[0] - mu_slope) / sig_slope) ** 2 + ((pars[1] - mu_intcpt) / sig_intcpt) ** 2)\n",
    "\n",
    "def ln_post(pars, x, y, var_noise, mu_slope, sig_slope, mu_intcpt, sig_intcpt):\n",
    "    return ln_like(pars, x, y, var_noise) + \\\n",
    "           ln_prior(pars, mu_slope, sig_slope, mu_intcpt, sig_intcpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the posterior on a grid of parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a smaller data realization\n",
    "x = x_grid(100)\n",
    "pars, y = simulator(x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, mu_var_noise)\n",
    "\n",
    "# and calculate its parameter posterior on a grid\n",
    "n_grid = 300\n",
    "slope_grid = np.linspace(mu_slope - 3.0 * sig_slope, mu_slope + 3.0 * sig_slope, n_grid)\n",
    "intcpt_grid = np.linspace(mu_intcpt - 3.0 * sig_intcpt, mu_intcpt + 3.0 * sig_intcpt, n_grid)\n",
    "post = np.zeros((n_grid, n_grid))\n",
    "for i in range(n_grid):\n",
    "    for j in range(n_grid):\n",
    "        post[i, j] = np.exp(ln_post([slope_grid[i], intcpt_grid[j]], x, y, mu_var_noise, mu_slope, \\\n",
    "                                    sig_slope, mu_intcpt, sig_intcpt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the joint posterior and the two one-dimensional marginal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate marginal posteriors\n",
    "post_slope = np.sum(post, axis=1)\n",
    "post_intcpt = np.sum(post, axis=0)\n",
    "\n",
    "# plot joint and marginal posteriors\n",
    "fig, axes = mp.subplots(2, 2, figsize=(8, 8))\n",
    "levels = np.array([0.011109, 0.13533528, 0.60653066]) * np.max(post)\n",
    "axes[0, 0].plot(slope_grid, post_slope, color='k')\n",
    "axes[1, 1].plot(intcpt_grid, post_intcpt, color='k')\n",
    "axes[1, 0].contour(slope_grid, intcpt_grid, post.T, levels=levels, colors='k')\n",
    "\n",
    "# prettify\n",
    "axes[0, 0].axvline(pars[0])\n",
    "axes[0, 0].set_xticklabels([])\n",
    "axes[0, 0].set_yticklabels([])\n",
    "axes[0, 0].set_xlim(axes[1, 0].get_xlim())\n",
    "axes[1, 0].axvline(pars[0])\n",
    "axes[1, 0].axhline(pars[1])\n",
    "axes[1, 0].set_xlabel(par_names[0])\n",
    "axes[1, 0].set_ylabel(par_names[1])\n",
    "axes[1, 1].set_xlim(axes[1, 0].get_ylim())\n",
    "axes[1, 1].axvline(pars[1])\n",
    "axes[1, 1].set_yticklabels([])\n",
    "axes[1, 1].set_xlabel(par_names[1])\n",
    "fig.delaxes(axes[0, 1])\n",
    "fig.subplots_adjust(hspace=0, wspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful way of summarizing the plots above is to determine the highest density region. One can do this by sorting each marginal probability array in decreasing order and finding the index at which the cumulative sum exceeds the desired threshold (e.g., 68%). The preceding indices define the highest density region: its limits in parameter space are a credible interval for the parameter of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very hacky way to find highest density region\n",
    "def cred_int(par, post, threshold):\n",
    "    \n",
    "    # sort the posterior and parameter arrays in order of decreasing probability\n",
    "    ind_sort = np.argsort(-post)\n",
    "    par_sort = par[ind_sort]\n",
    "    post_sort = post[ind_sort]\n",
    "    \n",
    "    # form the cumulative sum of the sorted posterior array, rescale from 0->1 \n",
    "    # and find the indices for whom the cumsum is less than the desired threshold\n",
    "    post_sort_cs = np.cumsum(post_sort) / np.sum(post)\n",
    "    ind_cred_int = post_sort_cs <= threshold\n",
    "    \n",
    "    # use these indices and the sorted parameter array to find the parameter values in\n",
    "    # the credible interval. the extremes of these values define the credible interval\n",
    "    par_in_cred_int = par_sort[ind_cred_int]\n",
    "    \n",
    "    return np.array([np.min(par_in_cred_int), np.max(par_in_cred_int)])\n",
    "\n",
    "print(cred_int(slope_grid, post_slope, 0.68))\n",
    "print(cred_int(intcpt_grid, post_intcpt, 0.68))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "First complicate the simulator so that we draw a random noise variance (which we now want to infer from the data) along with the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw sample from parameter prior\n",
    "def sample_prior_nv(mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    pars = np.zeros(3)\n",
    "    pars[0] = npr.normal(mu_slope, sig_slope)\n",
    "    pars[1] = npr.normal(mu_intcpt, sig_intcpt)\n",
    "    pars[2] = 1.0 / npr.gamma(shape_var_noise, 1.0 / scale_var_noise)\n",
    "    \n",
    "    return pars\n",
    "\n",
    "# generate noisy data given parameters\n",
    "def sample_data_nv(x, pars):\n",
    "    \n",
    "    y = pars[0] * x + pars[1] + npr.normal(0.0, np.sqrt(pars[2]), len(x))\n",
    "    \n",
    "    return y\n",
    "\n",
    "# more advanced simulator function: noisy measurements at known locations with unknown noise variance\n",
    "def simulator_nv(x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    # draw parameters from prior\n",
    "    pars = sample_prior_nv(mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise)\n",
    "    \n",
    "    # generate data\n",
    "    y = sample_data_nv(x, pars)\n",
    "    \n",
    "    return [pars, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's quickly check we're drawing parameters correctly from the prior..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw from priors\n",
    "pars = []\n",
    "for i in range(10000):\n",
    "    pars.append(sample_prior_nv(mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise))\n",
    "pars = np.array(pars)\n",
    "par_names_nv = [r'm', r'c', r'\\sigma_{\\rm noise}^2']\n",
    "\n",
    "# plot versus expectations\n",
    "print('parameter means & standard deviations')\n",
    "fig, axes = mp.subplots(1, 3, figsize=(16, 5))\n",
    "for i in range(3):\n",
    "    print('*', par_names_nv[i])\n",
    "    print('  reals:  ', np.mean(pars[:, i]), np.std(pars[:, i]))\n",
    "    _ = axes[i].hist(pars[:, i], bins=30, density=True)\n",
    "    par_lims = axes[i].get_xlim()\n",
    "    par_grid = np.linspace(par_lims[0], par_lims[1], 100)\n",
    "    axes[i].set_xlabel('$' + par_names_nv[i] + '$')\n",
    "    axes[i].set_ylabel(r'${\\rm P(}' + par_names_nv[i] + ')$')\n",
    "    if i == 0:\n",
    "        print('  theory: ', mu_slope, sig_slope)\n",
    "        axes[i].plot(par_grid, sps.norm.pdf(par_grid, mu_slope, sig_slope))\n",
    "    elif i == 1:\n",
    "        print('  theory: ', mu_intcpt, sig_intcpt)\n",
    "        axes[i].plot(par_grid, sps.norm.pdf(par_grid, mu_intcpt, sig_intcpt))\n",
    "    elif i == 2:\n",
    "        print('  theory: ', mu_var_noise, np.sqrt(var_var_noise))\n",
    "        axes[i].plot(par_grid, sps.invgamma.pdf(par_grid, shape_var_noise, scale=scale_var_noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and generating data realisations with the right noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a large data realization with fixed noise variance\n",
    "x = x_grid(10000)\n",
    "pars, y = simulator_nv(x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise)\n",
    "noise = y - (pars[0] * x + pars[1])\n",
    "\n",
    "# plot noise histogram versus expectation\n",
    "fig, axes = mp.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(x, y)\n",
    "axes[0].plot(x, pars[0] * x + pars[1])\n",
    "axes[0].set_xlabel('$x$')\n",
    "axes[0].set_ylabel('$y$')\n",
    "_ = axes[1].hist(noise, bins=30, density=True)\n",
    "par_lims = axes[1].get_xlim()\n",
    "par_grid = np.linspace(par_lims[0], par_lims[1], 100)\n",
    "axes[1].plot(par_grid, sps.norm.pdf(par_grid, 0.0, np.sqrt(pars[2])))\n",
    "axes[1].set_xlabel('$n$')\n",
    "axes[1].set_ylabel(r'${\\rm P(}n)$')\n",
    "print('noise mean & standard deviation')\n",
    "print('  reals:  ', np.mean(noise), np.std(noise))\n",
    "print('  theory: ', 0.0, np.sqrt(pars[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Metropolis-Hastings algorithm is a particularly simple and hence extremely broadly employed Markov-Chain Monte Carlo (MCMC) technique for obtaining samples from a posterior. All this algorithm requires is the ability to evaluate the posterior at any point in parameter space (up to any parameter-independent constants such as the *evidence*), along with a distribution from which to propose movements in parameter space. It is therefore typically the first option considered for settings in which the posterior moments can not be calculated analytically or gridded.\n",
    "\n",
    "The next task is to write down this sampler for our current setting: estimating the joint posterior of the slope and intercept, assuming the noise variance is not known. As before: let's split this into two parts. First, modify your three functions evaluating the log-likelihood, log-prior and log-posterior for the new setting. You must rewrite both the likelihood and prior functions in this case: the latter for obvious reasons; the former because we previously dropped the $\\sigma_{\\rm noise}$-dependent determinant term. We must also modify the posterior function, as we need to gracefully handle the sampler proposing negative (i.e., forbidden) values for the variance. We can do so by checking the proposed parameters (e.g., `if pars[2] <= 0.0`) and returning a large negative log-probability if a problem was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_like_nv(pars, x, y):\n",
    "    \n",
    "    delta = y - (pars[0] * x + pars[1])\n",
    "    cinv = np.diag(np.ones(len(x)) / pars[2])\n",
    "    ln_like_y = -0.5 * np.dot(np.dot(delta, cinv), delta)\n",
    "    ln_sqrt_det = -0.5 * len(x) * np.log(pars[2])\n",
    "    \n",
    "    return ln_like_y + ln_sqrt_det\n",
    "\n",
    "def ln_prior_nv(pars, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    ln_prior_reg = -0.5 * (((pars[0] - mu_slope) / sig_slope) ** 2 + ((pars[1] - mu_intcpt) / sig_intcpt) ** 2)\n",
    "    ln_prior_noise = (-shape_var_noise - 1.0) * np.log(pars[2]) - scale_var_noise / pars[2]\n",
    "    \n",
    "    return ln_prior_reg + ln_prior_noise\n",
    "\n",
    "def ln_post_nv(pars, x, y, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    # proceed only if variance is positive\n",
    "    if pars[2] <= 0.0:\n",
    "        return -1.0e90\n",
    "    else:\n",
    "        return ln_like_nv(pars, x, y) + \\\n",
    "               ln_prior_nv(pars, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write the sampler itself: choose a random starting point, then propose a move in parameter space and accept it with the appropriate probability. This requires a proposal distribution. Let's use a multivariate normal, centred on the current sample, with a user-specified covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_sampler(n_samples, x, y, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, \\\n",
    "               scale_var_noise, prop_cov):\n",
    "    \n",
    "    # setup\n",
    "    samples = np.zeros((n_samples, 3))\n",
    "    ln_posts = np.zeros(n_samples)\n",
    "    \n",
    "    # draw random starting point from prior\n",
    "    samples[0, :] = sample_prior_nv(mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, \\\n",
    "                                    scale_var_noise)\n",
    "    ln_posts[0] = ln_post_nv(samples[0, :], x, y, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                             shape_var_noise, scale_var_noise)\n",
    "    \n",
    "    # MH algorithm\n",
    "    for i in range(n_samples - 1):\n",
    "        \n",
    "        # propose new point in parameter space\n",
    "        proposal = npr.multivariate_normal(samples[i, :], prop_cov)\n",
    "        \n",
    "        # calculate log-posterior\n",
    "        ln_post_prop = ln_post_nv(proposal, x, y, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                                  shape_var_noise, scale_var_noise)\n",
    "        \n",
    "        # calculate acceptance probability:\n",
    "        ratio = np.exp(ln_post_prop - ln_posts[i])\n",
    "        \n",
    "        # accept or reject?\n",
    "        if ratio >= npr.rand():\n",
    "            samples[i + 1, :] = proposal\n",
    "            ln_posts[i + 1] = ln_post_prop\n",
    "        else:\n",
    "            samples[i + 1, :] = samples[i, :]\n",
    "            ln_posts[i + 1] = ln_posts[i]\n",
    "    \n",
    "    return samples, ln_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a reasonable-sized dataset and run the sampler! To do so, we need to pick a proposal covariance. Let's pick something very simple (and therefore suboptimal): a diagonal matrix with $\\sigma$ = 0.02 for the slope and intercept, and $\\sigma$ = 0.002 for the noise variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a smaller data realization\n",
    "x = x_grid(100)\n",
    "pars, y = simulator_nv(x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise)\n",
    "\n",
    "# define proposal covariance and sample!\n",
    "prop_cov = np.diag([0.02, 0.02, 0.002]) ** 2\n",
    "npr.seed(1234)\n",
    "samples, ln_posts = mh_sampler(50000, x, y, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, \\\n",
    "                               scale_var_noise, prop_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D plot showing the slope and intercept samples (the Markov chain) generated. Note the *burn-in* period, in which the sampler hones in on the region of appreciable posterior density from its initial random position. We can discard these samples when estimating the properties of the posterior. Add a so-called *trace plot* of the slope samples showing how the sampler generates correlated samples. Reducing this correlation would result in a (potentially significantly) more efficient sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = mp.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(samples[:, 0], samples[:, 1])\n",
    "axes[0].set_xlabel(par_names_nv[0])\n",
    "axes[0].set_ylabel(par_names_nv[1])\n",
    "axes[1].plot(samples[:, 0])\n",
    "axes[1].set_xlabel('sample number')\n",
    "axes[1].set_ylabel(par_names_nv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some marginalized parameter posteriors (by simply discarding all samples of the latent parameters) using DFM's [`corner`](https://corner.readthedocs.io/en/latest/) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "n_burnin = 100\n",
    "final_samples = samples[n_burnin:, :]\n",
    "par_names_nv_co = ['$' + par_name + '$' for par_name in par_names_nv]\n",
    "fig = corner.corner(final_samples, labels=par_names_nv_co, show_titles=True, truths=pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to play around with this simple sampler. Let's do a quick demonstration of the importance of the proposal distribution. For our chosen proposal, the covariance matrix tells the sampler how to try stepping around the posterior. The efficiency of this process strongly depends on how closely the proposal distribution matches the target posterior.\n",
    "\n",
    "Try out a couple of other choices for the proposal--one larger and one smaller than the current value--and see how the sampler performs. Plotting both the 2D samples and the slope trace plot will help us investigate the sampler's behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "mult = np.array([0.01, 1000.0])\n",
    "fig, axes = mp.subplots(1, 2, figsize=(12, 5))\n",
    "for i in range(2):\n",
    "    prop_cov_i = prop_cov * mult[i]\n",
    "    npr.seed(1234)\n",
    "    samples, ln_posts = mh_sampler(n_samples, x, y, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                                   shape_var_noise, scale_var_noise, prop_cov_i)\n",
    "    axes[0].plot(samples[:, 0], samples[:, 1], label=r'$\\sigma_{\\rm prop}\\times$'+'{:g}'.format(mult[i]))\n",
    "    axes[1].plot(samples[:, 0], label=r'$\\sigma_{\\rm prop}\\times$'+'{:g}'.format(mult[i]))\n",
    "axes[0].set_xlabel(par_names_nv[0])\n",
    "axes[0].set_ylabel(par_names_nv[1])\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[1].set_xlabel('sample number')\n",
    "axes[1].set_ylabel(par_names_nv[0])\n",
    "axes[1].legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the proposal $\\sigma$ is too small, the sampler makes very small steps around the posterior, producing highly correlated samples. If $\\sigma$ is too large, the sampler rejects most proposals, remaining stationary most of the time. In both cases the sampler is inefficient, taking an unnecessarily large number of samples to explore the posterior. Tuning the proposal is therefore an integral part of the MH algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Having to tune a large number of parameters to make your sampler efficient is annoying. [emcee](https://emcee.readthedocs.io/en/stable/) implements an affine-invariant ensemble sampling algorithm that reduces the tuning needed for efficient exploration of an $N$-dimensional posterior from $\\sim N^2/2$ parameters (i.e., a covariance matrix) to just 2, thereby enabling sampling in larger numbers of dimensions than Metropolis-Hastings. It also has a particularly simple interface, whereby the user only has to specify a function calculating the log-posterior (in exactly the format we've developed above, hooray!), a number of walkers to use and an initial position for each walker (which we can draw from the prior). Plug our problem into emcee: it's dead easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import emcee\n",
    "n_walkers = 50\n",
    "n_dim = len(pars)\n",
    "\n",
    "# define an initial position in parameter space for each walker: just draw from sample_prior_nv()\n",
    "init_pos = np.zeros((n_walkers, n_dim))\n",
    "for i in range(n_walkers):\n",
    "    init_pos[i, :] = sample_prior_nv(mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, \\\n",
    "                                     scale_var_noise)\n",
    "\n",
    "# create an emcee sampler. args here is a list of arguments required by the log-posterior function:\n",
    "# everything but the sampled parameter values\n",
    "args = [x, y, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise]\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, ln_post_nv, args=args)\n",
    "\n",
    "# sample. we do two runs here: one short to burn the sampler in, and one long to explore the posterior\n",
    "pos, prob, state = sampler.run_mcmc(init_pos, 200)\n",
    "print('* burned in')\n",
    "sampler.reset()\n",
    "pos, prob, state = sampler.run_mcmc(pos, 1000, rstate0=state)\n",
    "print('* sampling complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the samples we've generated and compare to the Metropolis-Hastings results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = sampler.flatchain\n",
    "fig = corner.corner(flat_samples, labels=par_names_nv_co, show_titles=True, truths=pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Move on to the problem in which there is uncertainty in the measurement positions. This a significantly harder inference problem, as we must infer the measurement positions as well as the regression parameters. First, re-write the simulator to generate uncertain measurement positions. Consider a (much!) smaller dataset so as to not overwhelm emcee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior settings\n",
    "n_x = 10\n",
    "mu_x = np.linspace(0.0, 1.0, n_x)\n",
    "sig_x = 0.025\n",
    "sig_noise_x = 0.05\n",
    "\n",
    "# draw sample from parameter prior: true x positions, slope, intercept & noise variance\n",
    "def prior_sample_nvx(mu_x, sig_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, \\\n",
    "                     scale_var_noise):\n",
    "    \n",
    "    n_x = len(mu_x)\n",
    "    pars = np.zeros(n_x + 3)\n",
    "    pars[0: n_x] = npr.normal(mu_x, sig_x, n_x)\n",
    "    pars[n_x] = npr.normal(mu_slope, sig_slope)\n",
    "    pars[n_x + 1] = npr.normal(mu_intcpt, sig_intcpt)\n",
    "    pars[n_x + 2] = 1.0 / npr.gamma(shape_var_noise, 1.0 / scale_var_noise)\n",
    "    \n",
    "    return pars\n",
    "\n",
    "# generate noisy data given parameters: noisy measurement positions and measurements\n",
    "def sample_data_nvx(pars, sig_noise_x):\n",
    "    \n",
    "    # dimension output array\n",
    "    n_x = len(pars) - 3\n",
    "    y = np.zeros(2 * n_x)\n",
    "    \n",
    "    # add noise to true measurement positions (pars[0: n_x])\n",
    "    y[0: n_x] = npr.normal(pars[0: n_x], sig_noise_x, n_x)\n",
    "    \n",
    "    # generate noisy measurements\n",
    "    y[n_x:] = pars[n_x] * pars[0: n_x] + pars[n_x + 1] + npr.normal(0.0, np.sqrt(pars[n_x + 2]), n_x)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# more advanced simulator function: noisy measurements at known locations with unknown noise variance\n",
    "def simulator_nvx(mu_x, sig_x, sig_noise_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                  shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    # draw parameters from prior\n",
    "    pars = prior_sample_nvx(mu_x, sig_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, \\\n",
    "                            scale_var_noise)\n",
    "    \n",
    "    # generate data\n",
    "    y = sample_data_nvx(pars, sig_noise_x)\n",
    "    \n",
    "    return [pars, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model is more complex I've not plotted all of the prior draws and noise histograms as before. Feel free to plot them if desired. For now, draw a single test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars_nvx, y_nvx = simulator_nvx(mu_x, sig_x, sig_noise_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                                shape_var_noise, scale_var_noise)\n",
    "mp.errorbar(y_nvx[0: n_x], y_nvx[n_x:], yerr=np.sqrt(pars_nvx[n_x + 2]), xerr=sig_noise_x)\n",
    "mp.xlabel('$x$')\n",
    "mp.ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make the changes required for inference with emcee..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_like_nvx(pars_x, pars_reg, y_x, y_reg, sig_noise_x):\n",
    "    \n",
    "    delta = y - (pars[0] * x + pars[1])\n",
    "    cinv = np.diag(np.ones(len(x)) / pars[2])\n",
    "    ln_like_y = -0.5 * np.dot(np.dot(delta, cinv), delta)\n",
    "    ln_sqrt_det = -0.5 * len(x) * np.log(pars[2])\n",
    "    ln_like_x = -0.5 * np.sum(((pars_x - y_x) / sig_noise_x) ** 2)\n",
    "    \n",
    "    return ln_like_y + ln_sqrt_det + ln_like_x\n",
    "\n",
    "def ln_prior_nvx(pars_x, pars_reg, mu_x, sig_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                 shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    ln_prior_reg = -0.5 * (((pars_reg[0] - mu_slope) / sig_slope) ** 2 + \\\n",
    "                           ((pars_reg[1] - mu_intcpt) / sig_intcpt) ** 2)\n",
    "    ln_prior_noise = (-shape_var_noise - 1.0) * np.log(pars_reg[2]) - scale_var_noise / pars_reg[2]\n",
    "    ln_prior_x = -0.5 * np.sum(((pars_x - mu_x) / sig_x) ** 2)\n",
    "    \n",
    "    return ln_prior_reg + ln_prior_noise + ln_prior_x\n",
    "\n",
    "def ln_post_nvx(pars, y, mu_x, sig_x, sig_noise_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    # split parameters and observations\n",
    "    n_pos = len(mu_x)\n",
    "    pars_x = pars[0: n_pos]\n",
    "    pars_reg = pars[n_pos:]\n",
    "    y_x = y[0: n_pos]\n",
    "    y_reg = y[n_pos:]\n",
    "    \n",
    "    # proceed only if variance is positive\n",
    "    if pars_reg[2] <= 0.0:\n",
    "        return -1.0e90\n",
    "    else:\n",
    "        return ln_like_nvx(pars_x, pars_reg, y_x, y_reg, sig_noise_x) + \\\n",
    "               ln_prior_nvx(pars_x, pars_reg, mu_x, sig_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                            shape_var_noise, scale_var_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and see what happens when run on our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "n_dim = n_x + 3\n",
    "\n",
    "# define an initial position in parameter space for each walker: just draw from sample_prior_nv()\n",
    "init_pos = np.zeros((n_walkers, n_dim))\n",
    "for i in range(n_walkers):\n",
    "    init_pos[i, :] = prior_sample_nvx(mu_x, sig_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                                      shape_var_noise, scale_var_noise)\n",
    "    \n",
    "# create an emcee sampler. args here is a list of arguments required by the log-posterior function:\n",
    "# everything but the sampled parameter values\n",
    "args = [y_nvx, mu_x, sig_x, sig_noise_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise]\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, ln_post_nvx, args=args)\n",
    "\n",
    "# sample. we do two runs here: one short to burn the sampler in, and one long to explore the posterior\n",
    "pos, prob, state = sampler.run_mcmc(init_pos, 200)\n",
    "print('* burned in')\n",
    "sampler.reset()\n",
    "pos, prob, state = sampler.run_mcmc(pos, 1000, rstate0=state)\n",
    "print('* sampling complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEGA PLOT INCOMING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_names_nvx = ['x_{:d}'.format(i) for i in range(n_x)] + par_names_nv\n",
    "par_names_nvx_co = ['$' + par_name + '$' for par_name in par_names_nvx]\n",
    "flat_samples = sampler.flatchain\n",
    "fig = corner.corner(flat_samples, labels=par_names_nvx_co, show_titles=True, truths=pars_nvx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many problems in Astrophysics with 10 datapoints. Let's do that again with more data to see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 200\n",
    "n_walkers = 420\n",
    "mu_x = np.linspace(0.0, 1.0, n_x)\n",
    "pars_nvx, y_nvx = simulator_nvx(mu_x, sig_x, sig_noise_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                                shape_var_noise, scale_var_noise)\n",
    "n_dim = n_x + 3\n",
    "init_pos = np.zeros((n_walkers, n_dim))\n",
    "for i in range(n_walkers):\n",
    "    init_pos[i, :] = prior_sample_nvx(mu_x, sig_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                                      shape_var_noise, scale_var_noise)\n",
    "args = [y_nvx, mu_x, sig_x, sig_noise_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, shape_var_noise, scale_var_noise]\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, ln_post_nvx, args=args)\n",
    "pos, prob, state = sampler.run_mcmc(init_pos, 200)\n",
    "print('* burned in')\n",
    "sampler.reset()\n",
    "pos, prob, state = sampler.run_mcmc(pos, 1000, rstate0=state)\n",
    "print('* sampling complete')\n",
    "flat_samples = sampler.flatchain\n",
    "fig = corner.corner(flat_samples[:, n_x:], labels=par_names_nv_co, show_titles=True, truths=pars_nvx[n_x:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the walltime is not ridiculous by any means (as the posterior is very Gaussian in most dimensions), but the quality of the sampling is breaking down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "Below I've written all but one of the conditional distributions required to infer the parameters of our most complicated model via Gibbs sampling. Please code up the remaining conditional, using the results you've derived on paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_x(n_x, pars, y, sig_noise_x, mu_x, sig_x):\n",
    "    \n",
    "    sample = np.zeros(n_x)\n",
    "    for i in range(n_x):\n",
    "        x_var = 1.0 / (1.0 / sig_x ** 2 + pars[n_x] ** 2 / pars[n_x + 2] + 1.0 / sig_noise_x ** 2)\n",
    "        x_mean = (mu_x[i] / sig_x ** 2 + \\\n",
    "                  pars[n_x] ** 2 / pars[n_x + 2] * (y[n_x + i] - pars[n_x + 1]) / pars[n_x] + \\\n",
    "                  y[i] / sig_noise_x ** 2) * x_var\n",
    "        sample[i] = npr.normal(x_mean, np.sqrt(x_var))\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def sample_slope(n_x, pars, y, mu_slope, sig_slope):\n",
    "    \n",
    "    slope_var = 1.0 / (np.sum(pars[0: n_x] ** 2 / pars[n_x + 2]) + 1.0 / sig_slope ** 2)\n",
    "    slope_mean = (np.sum(pars[0: n_x] ** 2 / pars[n_x + 2] * (y[n_x:] - pars[n_x + 1]) / pars[0: n_x]) + \\\n",
    "                  mu_slope / sig_slope ** 2) * slope_var\n",
    "    sample = npr.normal(slope_mean, np.sqrt(slope_var))\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def sample_intcpt(n_x, pars, y, mu_intcpt, sig_intcpt):\n",
    "    \n",
    "    intcpt_var = 1.0 / (n_x / pars[n_x + 2] + 1.0 / sig_intcpt ** 2)\n",
    "    intcpt_mean = (np.sum(1.0 / pars[n_x + 2] * (y[n_x:] - pars[n_x] * pars[0: n_x])) + \\\n",
    "                  mu_intcpt / sig_intcpt ** 2) * intcpt_var\n",
    "    sample = npr.normal(intcpt_mean, np.sqrt(intcpt_var))\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def sample_noise_var(n_x, pars, y, shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    new_shape = shape_var_noise + float(n_x) / 2.0\n",
    "    new_scale = scale_var_noise + 0.5 * np.sum((y[n_x:] - pars[n_x] * pars[0: n_x] - pars[n_x + 1]) ** 2)\n",
    "    sample = 1.0 / npr.gamma(new_shape, 1.0 / new_scale)\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write the Gibbs sampler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sample(y, mu_x, sig_x, sig_noise_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                 shape_var_noise, scale_var_noise):\n",
    "    \n",
    "    # storage\n",
    "    n_x = len(mu_x)\n",
    "    n_par = n_x + 3\n",
    "    samples = np.zeros((n_par, n_samples))\n",
    "    \n",
    "    # initialize sampler\n",
    "    samples[:, 0] = prior_sample_nvx(mu_x, sig_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                                     shape_var_noise, scale_var_noise)\n",
    "    \n",
    "    # sample!\n",
    "    sample = samples[:, 0]\n",
    "    for i in range(1, n_samples):\n",
    "        \n",
    "        # sample x\n",
    "        sample[0: n_x] = sample_x(n_x, sample, y, sig_noise_x, mu_x, sig_x)\n",
    "        \n",
    "        # sample slope\n",
    "        sample[n_x] = sample_slope(n_x, sample, y, mu_slope, sig_slope)\n",
    "        \n",
    "        # sample intercept\n",
    "        sample[n_x + 1] = sample_intcpt(n_x, sample, y, mu_intcpt, sig_intcpt)\n",
    "            \n",
    "        # sample noise variance\n",
    "        sample[n_x + 2] = sample_noise_var(n_x, sample, y, shape_var_noise, scale_var_noise)\n",
    "        \n",
    "        # store new state\n",
    "        samples[:, i] = sample\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sample, setting aside the first half of the samples as warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "all_samples = gibbs_sample(y_nvx, mu_x, sig_x, sig_noise_x, mu_slope, sig_slope, mu_intcpt, sig_intcpt, \\\n",
    "                           shape_var_noise, scale_var_noise)\n",
    "n_warmup = int(n_samples / 2)\n",
    "g_samples = all_samples[:, n_warmup:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the noise variance is being inferred as expected. First, generate a trace plot of the samples, overlaying the ground truth. Then print out the mean and standard deviation of the marginalized absolute magnitude posterior. Recall that marginalizing is as simple as throwing away the samples of all other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(g_samples[n_x + 2, :])\n",
    "mp.axhline(pars_nvx[n_x + 2], color='C1')\n",
    "mp.xlabel('sample')\n",
    "mp.ylabel(par_names_nv_co[2])\n",
    "print('Truth {:6.4f}; inferred {:6.4f} +/- {:6.4f}'.format(pars_nvx[n_x + 2], \\\n",
    "                                                           np.mean(g_samples[n_x + 2, :]), \\\n",
    "                                                           np.std(g_samples[n_x + 2, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some marginalized parameter posteriors (by simply discarding all samples of the latent parameters) using DFM's [`corner`](https://corner.readthedocs.io/en/latest/) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "samples = g_samples[n_x:, :]\n",
    "fig = corner.corner(samples.T, labels=par_names_nv_co,\n",
    "                    show_titles=True, truths=pars_nvx[n_x:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "\n",
    "The final task is to write a [Stan model](https://pystan.readthedocs.io/en/latest/getting_started.html) to infer the parameters of our linear regression problem. I've coded up the other two blocks required (`data` and `parameters`), so all that is required is for you to write the joint posterior (factorized into its individual components) in Stan's sampling-statement-based syntax. Essentially all you need are Gaussian sampling statements (`slope ~ normal(mu_slope, sig_slope);`) and for loops (`for(i in 1: n_x){...}`). A full list of the functions you might need can be found [here](https://mc-stan.org/docs/2_18/functions-reference/inverse-gamma-distribution.html).\n",
    "\n",
    "When you evaluate the cell below, Stan will translate your model into `c++` code and compile it. It's slow. We will then pickle the compiled model so you can re-use it rapidly without recompiling. To do so, please set `recompile = False` in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import sys\n",
    "import pystan as ps\n",
    "import pickle\n",
    "\n",
    "# stan model code\n",
    "stan_code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> n_x;\n",
    "    real y[2 * n_x];\n",
    "    real mu_x[n_x];\n",
    "    real sig_x;\n",
    "    real sig_noise_x;\n",
    "    real mu_slope;\n",
    "    real sig_slope;\n",
    "    real mu_intcpt;\n",
    "    real sig_intcpt;\n",
    "    real shape_var_noise;\n",
    "    real scale_var_noise;\n",
    "}\n",
    "parameters {\n",
    "    real x[n_x];\n",
    "    real slope;\n",
    "    real intcpt;\n",
    "    real var_noise;\n",
    "}\n",
    "model {\n",
    "    // priors\n",
    "    for(i in 1: n_x){\n",
    "        x[i] ~ normal(mu_x[i], sig_x);\n",
    "    }\n",
    "    slope ~ normal(mu_slope, sig_slope);\n",
    "    intcpt ~ normal(mu_intcpt, sig_intcpt);\n",
    "    var_noise ~ inv_gamma(shape_var_noise, scale_var_noise);\n",
    "    \n",
    "    // likelihoods\n",
    "    for(i in 1: n_x){\n",
    "        y[i] ~ normal(x[i], sig_noise_x);\n",
    "    }\n",
    "    for(i in 1: n_x){\n",
    "        y[n_x + i] ~ normal(slope * x[i] + intcpt, sqrt(var_noise));\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# compilation\n",
    "recompile = True\n",
    "pkl_fname = 'bhms_stan_model_v{:d}p{:d}p{:d}.pkl'.format(sys.version_info[0], \\\n",
    "                                                         sys.version_info[1], \\\n",
    "                                                         sys.version_info[2])\n",
    "if recompile:\n",
    "    stan_model = ps.StanModel(model_code=stan_code)\n",
    "    with open(pkl_fname, 'wb') as f:\n",
    "        pickle.dump(stan_model, f)\n",
    "else:\n",
    "    try:\n",
    "        with open(pkl_fname, 'rb') as f:\n",
    "            stan_model = pickle.load(f)\n",
    "    except EnvironmentError:\n",
    "        print('ERROR: pickled Stan model (' + pkl_fname + ') not found. ' + \\\n",
    "              'Please set recompile = True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_stan = 5000\n",
    "stan_data = {'n_x': n_x, 'y': y_nvx, 'mu_x': mu_x, 'sig_x': sig_x, 'sig_noise_x': sig_noise_x, \\\n",
    "             'mu_slope': mu_slope, 'sig_slope': sig_slope, 'mu_intcpt': mu_intcpt, 'sig_intcpt': sig_intcpt, \\\n",
    "             'shape_var_noise': shape_var_noise, 'scale_var_noise': scale_var_noise}\n",
    "fit = stan_model.sampling(data=stan_data, iter=n_samples_stan, chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... print out Stan's posterior summary (note this is for _all_ parameters)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fit.extract(permuted=True)\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the marginalized posterior of the non-nuisance parameters, as with the Gibbs sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_samples = np.stack((samples['slope'], samples['intcpt'], samples['var_noise']))\n",
    "fig = corner.corner(c_samples.T, labels=par_names_nv_co,\n",
    "                    show_titles=True, truths=pars_nvx[n_x:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our work here is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
