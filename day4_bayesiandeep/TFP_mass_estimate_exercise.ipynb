{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFP_mass_estimate_exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroHackWeek/AstroHackWeek2019/blob/master/day4_bayesiandeep/TFP_mass_estimate_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIKPkpeOR3-D",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2019 Matthew Ho, Francois Lanusse.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiP5vuHbSQVq",
        "colab_type": "text"
      },
      "source": [
        "# Estimating Galaxy Cluster Masses with TensorFlow Probability\n",
        "\n",
        "Authors: \n",
        "  - [@maho3](https://github.com/maho3) (Matt Ho)\n",
        "  - [@EiffL](https://github.com/EiffL) (Francois Lanusse)\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this tutorial, we learn how to combine Keras and TensorFlow Probability to estimate the masses of galaxy clusters using velocity dispersion measurements. See for instance [Ho et al. (2019)](https://arxiv.org/abs/1902.05950) for a state of the art machine learning approach to this problem.\n",
        "\n",
        "The context for this problem is to be able to build robust mass estimates for \n",
        "galaxy clusters, simply by using information from their galaxy members. And in \n",
        "particular line of sight velocity information which can be obtained by spectroscopy.\n",
        "\n",
        "As we will see, despite combining a number of input features, the model prediction always has some uncertainties and it is important to properly account for them to avoid biasing results.\n",
        "\n",
        "### Learning objectives\n",
        "\n",
        "In this notebook, we will learn:\n",
        "*   The limits of simple regression which can lead to biases.\n",
        "*   How to use TensorFlow Probability to build a probabilistic model.\n",
        "*   How to use the model probability predicted by the model to eliminate usual biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80MpR9d-RlHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Imports and Utility functions { display-mode: \"form\" }\n",
        "%pylab inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# Activate TF2 behavior:\n",
        "from tensorflow.python import tf2\n",
        "if not tf2.enabled():\n",
        "  import tensorflow.compat.v2 as tf\n",
        "  tf.enable_v2_behavior()\n",
        "  assert tf2.enabled()\n",
        "\n",
        "def binned_plot(X, Y, n=10, percentiles=[35, 50], ax=None, **kwargs):\n",
        "    # Calculation\n",
        "    calc_percent = []\n",
        "    for p in percentiles:\n",
        "        if p < 50:\n",
        "            calc_percent.append(50-p)\n",
        "            calc_percent.append(50+p)\n",
        "        elif p == 50:\n",
        "            calc_percent.append(50)\n",
        "        else:\n",
        "            raise Exception('Percentile > 50')\n",
        "\n",
        "    bin_edges = np.linspace(X.min()*0.9999, X.max()*1.0001, n+1)\n",
        "\n",
        "    dtype = [(str(i), 'f') for i in calc_percent]\n",
        "    bin_data = np.zeros(shape=(n,), dtype=dtype)\n",
        "\n",
        "    for i in range(n):\n",
        "        y = Y[(X >= bin_edges[i]) & (X < bin_edges[i+1])]\n",
        "\n",
        "        if len(y) == 0:\n",
        "            continue\n",
        "\n",
        "        y_p = np.percentile(y, calc_percent)\n",
        "\n",
        "        bin_data[i] = tuple(y_p)\n",
        "\n",
        "    # Plotting\n",
        "    if ax is None:\n",
        "        f, ax = plt.subplots()\n",
        "\n",
        "    bin_centers = [np.mean(bin_edges[i:i+2]) for i in range(n)]\n",
        "    for p in percentiles:\n",
        "        if p == 50:\n",
        "            ax.plot(bin_centers, bin_data[str(p)], **kwargs)\n",
        "        else:\n",
        "            ax.fill_between(bin_centers,\n",
        "                            bin_data[str(50-p)],\n",
        "                            bin_data[str(50+p)],\n",
        "                            alpha=0.2,\n",
        "                            **kwargs)\n",
        "\n",
        "    return bin_data, bin_edges\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOdxcnGYenbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checking for GPU access\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUpk4oK7T8sS",
        "colab_type": "text"
      },
      "source": [
        "## Understanding the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXqDYLU-cMLw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Loading dataset.\n",
        "#from google.colab import auth\n",
        "from astropy.table import Table\n",
        "\n",
        "#auth.authenticate_user()\n",
        "bucket_name='ahw2019'\n",
        "\n",
        "!gsutil cp gs://{bucket_name}/halo_mass_regression/'Rockstar_UM_z=0.117_contam_summary.fits' contam_summary.fits\n",
        "\n",
        "print('Download complete')\n",
        "\n",
        "data = Table.read('contam_summary.fits')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i6vcxqAgfNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's check the mass distribution in this sample\n",
        "hist(log10(data['M200c']),100)\n",
        "xlabel(r'$\\log[M_{200c}\\ (h^{-1}M_\\odot)]$')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU6bWyAdOmlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist2d(log10(data['M200c']), log10(data['Ngal']),64,cmap='gist_stern');\n",
        "plt.colorbar()\n",
        "xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')\n",
        "ylabel(r'$\\log_{10}[N_\\mathrm{gal}]$')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qA0S4LC58tl",
        "colab_type": "text"
      },
      "source": [
        "This plot shows the scaling relation between mass and number of cluster members"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC5oGVC2PNYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist2d(log10(data['M200c']), log10(data['sigv']),64,cmap='gist_stern');\n",
        "plt.colorbar()\n",
        "xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')\n",
        "ylabel(r'$\\log_{10}[\\sigma_v\\ (km\\ s^{-1})]$')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egX3KcY16EGI",
        "colab_type": "text"
      },
      "source": [
        "This plot illustrates the scaling relation between mass and the velocity dispersion of cluster members."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD7CVmu7i3mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing some features\n",
        "data['LogNgal'] = np.log10(data['Ngal'])\n",
        "data['Logsigv'] = np.log10(data['sigv'])\n",
        "data['logmass'] = np.log10(data['M200c'])\n",
        "\n",
        "# Preparing data set\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Split into train and test\n",
        "inds_random = permutation(len(data))\n",
        "data_train = data[inds_random[:200000]]\n",
        "data_test = data[inds_random[200000:]]\n",
        "\n",
        "# Features to use for regression\n",
        "X = data[['LogNgal', # log richness\n",
        "          'Logsigv', # log velocity dispersion\n",
        "          'R_mean', 'R_std', 'R_skew', 'R_kurt', # descriptive features of cluster member projected radius distribution\n",
        "          'm_mean', 'm_std', 'm_skew', 'm_kurt', # \" of member stellar mass distribution\n",
        "          'v_mean', 'v_std', 'v_skew', 'v_kurt'  # \" of member LOS velocity distribution\n",
        "         ]].to_pandas().values\n",
        "\n",
        "X_train = X[inds_random[:200000]]\n",
        "X_test = X[inds_random[200000:]]\n",
        "\n",
        "scaler = MinMaxScaler().fit(data['logmass'].reshape((-1,1)))\n",
        "feature_scaler = StandardScaler().fit(X_train)\n",
        "\n",
        "X_train = feature_scaler.transform(X_train)\n",
        "Y_train = np.clip(scaler.transform(data_train['logmass'].reshape((-1,1))), 1e-5,1-1e-5)\n",
        "\n",
        "X_test = feature_scaler.transform(X_test)\n",
        "Y_test = np.clip(scaler.transform(data_test['logmass'].reshape((-1,1))),1e-5,1-1e-5)\n",
        "\n",
        "Y_true = data_test['logmass']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6UWg6_mUCic",
        "colab_type": "text"
      },
      "source": [
        "## First approach: Regression\n",
        "\n",
        "We begin by implementing a simple regression model using a mean squared error loss. This is the standard approach taken by most ML papers on this problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smwgMzO9UHNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Exercise: Build a Dense Neural Network for doing regression, with something\n",
        "# Like 3 layers\n",
        "regression_model = keras.Sequential([\n",
        "    # Add your layers\n",
        "])\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(lambda e: 0.001 if e < 2 else 0.0001)\n",
        "\n",
        "regression_model.compile(loss='mean_squared_error', optimizer='adam', metrics=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wiu7CnFUIDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regression_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq2kAI8Di0Cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = regression_model.fit(X_train, Y_train, epochs=5, batch_size=100, callbacks=[callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF8ZJbhu6zUe",
        "colab_type": "text"
      },
      "source": [
        "Now that the model is trained, let's exrtact some predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLoZIqRDi0b_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = scaler.inverse_transform(regression_model.predict(X_test)).squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFHjAIMYi0i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = plt.figure(figsize=(6,8))\n",
        "gs = mpl.gridspec.GridSpec(2,1,height_ratios=[3,1], hspace=0)\n",
        "\n",
        "ax1 = f.add_subplot(gs[0,0])\n",
        "\n",
        "ax1.plot(np.arange(13,16,0.1),np.arange(13,16,0.1),'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                Y_pred, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax1)\n",
        "\n",
        "ax1.set_xlim(13.5,15.3)\n",
        "ax1.set_ylim(13.5,15.3)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks(ax1.get_yticks()[1:])\n",
        "ax1.set_ylabel(r'$\\log_{10}\\left[M_\\mathrm{pred}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "\n",
        "ax2 = f.add_subplot(gs[1,0])\n",
        "\n",
        "ax2.plot(np.arange(13,16,0.1),[0]*30,'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                Y_pred - Y_true, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax2)\n",
        "ax2.set_xlim(13.5,15.3)\n",
        "ax2.set_ylim(-0.5,0.5)\n",
        "ax2.set_xlabel(r'$\\log_{10}\\left[M_\\mathrm{true}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "ax2.set_ylabel(r'$\\epsilon$', fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEhBisG68Kv",
        "colab_type": "text"
      },
      "source": [
        "We see a significant bias... What could be the cause? What did we do wrong?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-xXGjBQUI4z",
        "colab_type": "text"
      },
      "source": [
        "## Second approach: Probabilistic Modelling\n",
        "\n",
        "By now we now that using a MSE can lead to biased estimates. How about we switch to a model of the full posterior, that should fix our problems!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jgpOmdJUMaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Exercise: Implement a Mixture Density Network to predict the cluster mass posterior\n",
        "model = keras.Sequential([\n",
        "                          ### Add  your layers\n",
        "])\n",
        "\n",
        "# This will be our log likelihood function\n",
        "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(lambda e: 0.001 if e < 5 else 0.0001)\n",
        "\n",
        "model.compile(loss=negloglik, optimizer='adam', metrics=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMaqavDFUMwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=10, batch_size=100, callbacks=[callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyBuY586as8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot(history.history['loss'])\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJLg6gKmMfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computing mean of the posterior, and output of the previous regression model\n",
        "# for comparison\n",
        "Y_pred = scaler.inverse_transform(model(X_test).mean().numpy()).squeeze()\n",
        "Y_pred_reg = scaler.inverse_transform(regression_model.predict(X_test)).squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORW15dhAmMaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = plt.figure(figsize=(6,8))\n",
        "gs = mpl.gridspec.GridSpec(2,1,height_ratios=[3,1], hspace=0)\n",
        "\n",
        "ax1 = f.add_subplot(gs[0,0])\n",
        "\n",
        "ax1.plot(np.arange(13,16,0.1),np.arange(13,16,0.1),'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                Y_pred, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax1)\n",
        "_ = binned_plot(Y_true, \n",
        "                Y_pred_reg, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='m', ax=ax1)\n",
        "\n",
        "ax1.set_xlim(13.5,15.3)\n",
        "ax1.set_ylim(13.5,15.3)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks(ax1.get_yticks()[1:])\n",
        "ax1.set_ylabel(r'$\\log_{10}\\left[M_\\mathrm{pred}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "\n",
        "\n",
        "ax2 = f.add_subplot(gs[1,0])\n",
        "\n",
        "ax2.plot(np.arange(13,16,0.1),[0]*30,'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                Y_pred - Y_true, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax2)\n",
        "_ = binned_plot(Y_true, \n",
        "                Y_pred_reg - Y_true, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='m', ax=ax2)\n",
        "ax2.set_xlim(13.5,15.3)\n",
        "ax2.set_ylim(-0.5,0.5)\n",
        "ax2.set_xlabel(r'$\\log_{10}\\left[M_\\mathrm{true}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "ax2.set_ylabel(r'$\\epsilon$', fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ5Yv0oynVgF",
        "colab_type": "text"
      },
      "source": [
        "Wait.... What? We get the same answer despite our fancy model. Why is that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weXAEq_G9AIK",
        "colab_type": "text"
      },
      "source": [
        "### What is my prior?\n",
        "\n",
        "As we have seen today, the output of the Mixture Density Network can be seen as a posterior distribution under the prior defined by the distribution of masses in the training set. \n",
        "\n",
        "Let's have a look at this distribution $p(M_{200c})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGQSBQgSnUe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.stats\n",
        "hist = np.histogram(scaler.inverse_transform(Y_train), 64)\n",
        "prior = scipy.stats.rv_histogram(hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOEtHY61nUjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(scaler.inverse_transform(Y_train), 100, normed=True);\n",
        "x = linspace(13.51,15.2,100)\n",
        "plot(x, prior.pdf(x))\n",
        "xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FbGMc__9vMs",
        "colab_type": "text"
      },
      "source": [
        "So it's definitely not flat, as a matter of fact, it's heavily  preferring lower mass halos. This could explain why our posterior masses are systematically predicted low when we take the mean of the posterior.\n",
        "\n",
        "Let's compute the mass PDF predicted by the model for  all clusters in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoIdhi_mnUYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This returns the distribution q(M | x) for all clusters\n",
        "outputs = model(X_test)\n",
        "xt = scaler.transform(x.reshape((-1,1)))\n",
        "logps = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "    logps.append(outputs.log_prob(xt[i]).numpy())\n",
        "logps = np.stack(logps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVvcr6H9-ZQz",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the prior and posterior distributions, we can replace the training set prior $\\tilde{p}$ by a flat prior $p$ simply by using the formula:\n",
        "$$ q(M_{200c} | x ) \\propto \\frac{\\tilde{p}(M_{200c})}{p(M_{200c})} p(M_{200c} | x) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWWQZE3drdBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  figure()\n",
        "  plot(x, np.exp(logps[:,-i]), label='posterior under training prior')\n",
        "  plot(x, np.exp(logps[:,-i])/prior.pdf(x), label='posterior under flat prior')\n",
        "  axvline(Y_true[-i], color='m', label='True value')\n",
        "  xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')\n",
        "  legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiUBYdFq_TR6",
        "colab_type": "text"
      },
      "source": [
        "Yep, the posteriors tend to move  to the right!\n",
        "\n",
        "Let's compute this 'corrected posterior' for all clusters, and use the new posterior mean as our point estimate of the mass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sZftwiBrc-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.integrate import simps\n",
        "corrected_posterior = np.exp(logps)/(prior.pdf(x).reshape((-1,1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUOAbW-qrc9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_prior_mean = simps(x.reshape((-1,1))*corrected_posterior, x,axis=0)/simps(corrected_posterior,x,axis=0 )  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hprBZk3rrc0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = plt.figure(figsize=(6,8))\n",
        "gs = mpl.gridspec.GridSpec(2,1,height_ratios=[3,1], hspace=0)\n",
        "\n",
        "ax1 = f.add_subplot(gs[0,0])\n",
        "\n",
        "ax1.plot(np.arange(13,16,0.1),np.arange(13,16,0.1),'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                y_pred_prior_mean, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax1)\n",
        "\n",
        "ax1.set_xlim(13.5,15.3)\n",
        "ax1.set_ylim(13.5,15.3)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks(ax1.get_yticks()[1:])\n",
        "ax1.set_ylabel(r'$\\log_{10}\\left[M_\\mathrm{pred}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "\n",
        "\n",
        "ax2 = f.add_subplot(gs[1,0])\n",
        "\n",
        "ax2.plot(np.arange(13.,16,0.1),[0]*30,'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                y_pred_prior_mean - Y_true, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax2)\n",
        "\n",
        "ax2.set_xlim(13.5,15.3)\n",
        "ax2.set_ylim(-0.5,0.5)\n",
        "ax2.set_xlabel(r'$\\log_{10}\\left[M_\\mathrm{true}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "ax2.set_ylabel(r'$\\epsilon$', fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nszn-gD6fHFR",
        "colab_type": "text"
      },
      "source": [
        "We got rid of most of bias \\o/ But what's going on at the edges???\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIRhtgUK_8y8",
        "colab_type": "text"
      },
      "source": [
        "### Is the posterior mean a good summary?\n",
        "\n",
        "Let's have a look at the predicted posterior for objects on the edge of the distribution to understand what is going on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFBq8kxRfUVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = Y_true < 13.55\n",
        "plot(x, corrected_posterior[:,m][:,0])\n",
        "axvline(Y_true[m][0], ls='--', label='true value', color='m')\n",
        "axvline(y_pred_prior_mean[m][0], label='posterior mean', color='r')\n",
        "legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJgtbD4NhZql",
        "colab_type": "text"
      },
      "source": [
        "Ok, so the problem was that the posterior distribution is not symmetrical near the edges, so the posterior mean ends up being very biased, what we want is the \n",
        "posterior mode!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5f95veMhZCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exercise: Find the mode of the posterior\n",
        "y_pred_mode = # Work your magic here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfH_-gt4rc7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = plt.figure(figsize=(6,8))\n",
        "gs = mpl.gridspec.GridSpec(2,1,height_ratios=[3,1], hspace=0)\n",
        "\n",
        "ax1 = f.add_subplot(gs[0,0])\n",
        "\n",
        "ax1.plot(np.arange(13,16,0.1),np.arange(13,16,0.1),'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                y_pred_mode, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax1)\n",
        "\n",
        "ax1.set_xlim(13.5,15.3)\n",
        "ax1.set_ylim(13.5,15.3)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks(ax1.get_yticks()[1:])\n",
        "ax1.set_ylabel(r'$\\log_{10}\\left[M_\\mathrm{pred}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "\n",
        "ax2 = f.add_subplot(gs[1,0])\n",
        "\n",
        "ax2.plot(np.arange(13,16,0.1),[0]*30,'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                y_pred_mode - Y_true, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax2)\n",
        "ax2.set_xlim(13.5,15.3)\n",
        "ax2.set_ylim(-0.5,0.5)\n",
        "ax2.set_xlabel(r'$\\log_{10}\\left[M_\\mathrm{true}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "ax2.set_ylabel(r'$\\epsilon$', fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9whjWa1uh7NW",
        "colab_type": "text"
      },
      "source": [
        "Tadaa! Works nicely, especially at lower masses, but not perfect at higher masses? \n",
        "The main reason is that we are correcting a posterior the prior used during training, a better strategy would be to train the model on a dataset with flat prior masses, as was done in Ho et al. (2019).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCUJjCIuB1Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
