{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AHW2019_photoz_inference_partI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroHackWeek/AstroHackWeek2019/blob/master/day4_bayesiandeep/photoz_inference_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSFB2g954QdI",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2019 Francois Lanusse.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnyjQvJ1E2-",
        "colab_type": "text"
      },
      "source": [
        "# Photoz with TensorFlow Probability - Part I: Data Preparation\n",
        "\n",
        "Author: [@EiffL](https://github.com/EiffL) (Francois Lanusse)\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this series of tutorials, we learn how to combine Keras, TensorFlow Probability, and Google Colab to train a photo-z inference in the cloud. This first notebook in the series focuses on data preparation.\n",
        "\n",
        "### Learning objectives\n",
        "\n",
        "In this notebook, we will learn how to:\n",
        "*   Setup a Google Cloud Bucket to store training data\n",
        "*   Query the HSC Survey public data release with [unagi](https://github.com/dr-guangtou/unagi)\n",
        "*   Export a dataset in TFRecord format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPKQaxoP5KiA",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg0GVUCV5JpT",
        "colab_type": "code",
        "outputId": "4ea18e8f-d633-4ca2-ad1e-046e8853eed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install git+https://github.com/eiffl/unagi.git\n",
        "import h5py\n",
        "import six\n",
        "import os\n",
        "from astropy.table import Table\n",
        "import astropy.units as u\n",
        "from unagi import hsc\n",
        "from unagi import task"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/eiffl/unagi.git\n",
            "  Cloning https://github.com/eiffl/unagi.git to /tmp/pip-req-build-4k6n46_0\n",
            "  Running command git clone -q https://github.com/eiffl/unagi.git /tmp/pip-req-build-4k6n46_0\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from unagi==0.1.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from unagi==0.1.0) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from unagi==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from unagi==0.1.0) (2.4.2)\n",
            "Requirement already satisfied: astropy>=3.0 in /usr/local/lib/python3.6/dist-packages (from unagi==0.1.0) (3.0.5)\n",
            "Requirement already satisfied: matplotlib>=2.0 in /usr/local/lib/python3.6/dist-packages (from unagi==0.1.0) (3.0.3)\n",
            "Collecting fits2hdf>=1.1.1 (from unagi==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/56/97e2d36f148ab76214439dcf9a3395a8ae00802078605188090e132057a2/fits2hdf-1.1.1-py3-none-any.whl\n",
            "Collecting tenacity>=5.1.1 (from unagi==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/a1/be8c8610f4620c56790965ba2b564dd76d13cbcd7c2ff8f6053ce63027fb/tenacity-5.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0->unagi==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0->unagi==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0->unagi==0.1.0) (2.5.3)\n",
            "Collecting colorama (from fits2hdf>=1.1.1->unagi==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from fits2hdf>=1.1.1->unagi==0.1.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0->unagi==0.1.0) (41.2.0)\n",
            "Building wheels for collected packages: unagi\n",
            "  Building wheel for unagi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unagi: filename=unagi-0.1.0-cp36-none-any.whl size=4175837 sha256=e6236858626946b902ee7a042f34d9f5addb7a5c7dd77e62b16f7faf35bce02f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gzn802xp/wheels/4c/39/e2/3565a57b9e302f5ea63c4e9680cbfab8f868256bc659c94532\n",
            "Successfully built unagi\n",
            "Installing collected packages: colorama, fits2hdf, tenacity, unagi\n",
            "Successfully installed colorama-0.4.1 fits2hdf-1.1.1 tenacity-5.1.1 unagi-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHXDvI31AA95",
        "colab_type": "text"
      },
      "source": [
        "## Setting up a Google Cloud Bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djr-saaxAfg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "bucket_name='ahw2019'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g32lCWGr4h5-",
        "colab_type": "text"
      },
      "source": [
        "## Downloading data from the HSC Survey\n",
        "\n",
        "Need an account on HSC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3GnoM5jeQ-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = '''\n",
        "-- Merge forced photometry and spectroscopic sample from HSC PDR 2 wide\n",
        "SELECT object_id, ra, dec, tract, patch,\n",
        "\t-- Absorption\n",
        "\ta_g, a_r, a_i, a_z, a_y,\n",
        "\t-- Extendedness\n",
        "\tg_extendedness_value, r_extendedness_value, i_extendedness_value, z_extendedness_value, y_extendedness_value,\n",
        "  -- Background Information\n",
        "  g_localbackground_flux, r_localbackground_flux, i_localbackground_flux, z_localbackground_flux, y_localbackground_flux,\n",
        "\t-- Fluxes\n",
        "\tg_cmodel_flux, g_cmodel_fluxsigma, g_cmodel_exp_flux, g_cmodel_exp_fluxsigma, g_cmodel_dev_flux, g_cmodel_dev_fluxsigma,\n",
        "\tr_cmodel_flux, r_cmodel_fluxsigma, r_cmodel_exp_flux, r_cmodel_exp_fluxsigma, r_cmodel_dev_flux, r_cmodel_dev_fluxsigma,\n",
        "\ti_cmodel_flux, i_cmodel_fluxsigma, i_cmodel_exp_flux, i_cmodel_exp_fluxsigma, i_cmodel_dev_flux, i_cmodel_dev_fluxsigma,\n",
        "\tz_cmodel_flux, z_cmodel_fluxsigma, z_cmodel_exp_flux, z_cmodel_exp_fluxsigma, z_cmodel_dev_flux, z_cmodel_dev_fluxsigma,\n",
        "\ty_cmodel_flux, y_cmodel_fluxsigma, y_cmodel_exp_flux, y_cmodel_exp_fluxsigma, y_cmodel_dev_flux, y_cmodel_dev_fluxsigma,\n",
        "\t-- Magnitudes\n",
        "\tg_cmodel_mag, g_cmodel_magsigma, g_cmodel_exp_mag, g_cmodel_exp_magsigma, g_cmodel_dev_mag, g_cmodel_dev_magsigma,\n",
        "\tr_cmodel_mag, r_cmodel_magsigma, r_cmodel_exp_mag, r_cmodel_exp_magsigma, r_cmodel_dev_mag, r_cmodel_dev_magsigma,\n",
        "\ti_cmodel_mag, i_cmodel_magsigma, i_cmodel_exp_mag, i_cmodel_exp_magsigma, i_cmodel_dev_mag, i_cmodel_dev_magsigma,\n",
        "\tz_cmodel_mag, z_cmodel_magsigma, z_cmodel_exp_mag, z_cmodel_exp_magsigma, z_cmodel_dev_mag, z_cmodel_dev_magsigma,\n",
        "\ty_cmodel_mag, y_cmodel_magsigma, y_cmodel_exp_mag, y_cmodel_exp_magsigma, y_cmodel_dev_mag, y_cmodel_dev_magsigma,\n",
        "\t-- Shapes\n",
        "\tg_sdssshape_shape11, g_sdssshape_shape12, g_sdssshape_shape22, g_sdssshape_psf_shape11, g_sdssshape_psf_shape12, g_sdssshape_psf_shape22,\n",
        "\tr_sdssshape_shape11, r_sdssshape_shape12, r_sdssshape_shape22, r_sdssshape_psf_shape11, r_sdssshape_psf_shape12, r_sdssshape_psf_shape22,\n",
        "\ti_sdssshape_shape11, i_sdssshape_shape12, i_sdssshape_shape22, i_sdssshape_psf_shape11, i_sdssshape_psf_shape12, i_sdssshape_psf_shape22,\n",
        "\tz_sdssshape_shape11, z_sdssshape_shape12, z_sdssshape_shape22, z_sdssshape_psf_shape11, z_sdssshape_psf_shape12, z_sdssshape_psf_shape22,\n",
        "\ty_sdssshape_shape11, y_sdssshape_shape12, y_sdssshape_shape22, y_sdssshape_psf_shape11, y_sdssshape_psf_shape12, y_sdssshape_psf_shape22,\n",
        "\t-- specz\n",
        "\td_pos, d_mag, specz_ra, specz_dec, specz_redshift, specz_redshift_err, specz_mag_i\n",
        "\n",
        "FROM pdr2_wide.forced forced\n",
        "  LEFT JOIN pdr2_wide.forced2 USING (object_id)\n",
        "  LEFT JOIN pdr2_wide.forced3 USING (object_id)\n",
        "\tINNER JOIN pdr2_wide.specz USING (object_id)\n",
        "\n",
        "-- Applying some data quality cuts\n",
        "WHERE forced.isprimary\n",
        "-- Keep only objects with reliable spectroscopic redshifts\n",
        "AND specz.specz_flag_homogeneous\n",
        "-- no stars, quasars, or failures\n",
        "AND specz.specz_redshift < 9 AND specz.specz_redshift > 0.01\n",
        "-- error cut\n",
        "AND specz.specz_redshift_err < 0.005*(1 + specz.specz_redshift)\n",
        "-- Keeping only the matches that fall within 0.2 arcsec\n",
        "AND specz.d_pos <= 0.2\n",
        "-- Simple Full Depth Full Colour cuts: At least 3 exposures in each band\n",
        "AND forced.g_inputcount_value >= 3\n",
        "AND forced.r_inputcount_value >= 3\n",
        "AND forced.i_inputcount_value >= 3\n",
        "AND forced.z_inputcount_value >= 3\n",
        "AND forced.y_inputcount_value >= 3\n",
        "-- Remove objects affected by bright stars\n",
        "AND NOT forced.g_pixelflags_bright_objectcenter\n",
        "AND NOT forced.r_pixelflags_bright_objectcenter\n",
        "AND NOT forced.i_pixelflags_bright_objectcenter\n",
        "AND NOT forced.z_pixelflags_bright_objectcenter\n",
        "AND NOT forced.y_pixelflags_bright_objectcenter\n",
        "AND NOT forced.g_pixelflags_bright_object\n",
        "AND NOT forced.r_pixelflags_bright_object\n",
        "AND NOT forced.i_pixelflags_bright_object\n",
        "AND NOT forced.z_pixelflags_bright_object\n",
        "AND NOT forced.y_pixelflags_bright_object\n",
        "-- Remove objects intersecting edges\n",
        "AND NOT forced.g_pixelflags_edge\n",
        "AND NOT forced.r_pixelflags_edge\n",
        "AND NOT forced.i_pixelflags_edge\n",
        "AND NOT forced.z_pixelflags_edge\n",
        "AND NOT forced.y_pixelflags_edge\n",
        "-- Remove objects with saturated or interpolated pixels\n",
        "AND NOT forced.g_pixelflags_saturatedcenter\n",
        "AND NOT forced.r_pixelflags_saturatedcenter\n",
        "AND NOT forced.i_pixelflags_saturatedcenter\n",
        "AND NOT forced.z_pixelflags_saturatedcenter\n",
        "AND NOT forced.y_pixelflags_saturatedcenter\n",
        "AND NOT forced.g_pixelflags_interpolatedcenter\n",
        "AND NOT forced.r_pixelflags_interpolatedcenter\n",
        "AND NOT forced.i_pixelflags_interpolatedcenter\n",
        "AND NOT forced.z_pixelflags_interpolatedcenter\n",
        "AND NOT forced.y_pixelflags_interpolatedcenter\n",
        "AND NOT forced.g_pixelflags_bad\n",
        "AND NOT forced.r_pixelflags_bad\n",
        "AND NOT forced.i_pixelflags_bad\n",
        "AND NOT forced.z_pixelflags_bad\n",
        "AND NOT forced.y_pixelflags_bad\n",
        "-- Remove objects with generic cmodel fit failures\n",
        "AND NOT forced.g_cmodel_flag\n",
        "AND NOT forced.r_cmodel_flag\n",
        "AND NOT forced.i_cmodel_flag\n",
        "AND NOT forced.z_cmodel_flag\n",
        "AND NOT forced.y_cmodel_flag\n",
        "-- Sort by tract and patch for faster cutout query\n",
        "ORDER BY object_id\n",
        ";\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqLcdztg-_o9",
        "colab_type": "text"
      },
      "source": [
        "### Download catalog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1edA1qN79Oic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Login\n",
        "archive = hsc.Hsc(dr='pdr2', rerun='pdr2_wide')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2fsZVb69TdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "catalog = archive.sql_query(query, from_file=False, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1aOeDWp_PpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the catalog\n",
        "catalog.write('catalog.fits')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn4A-oyzxhyk",
        "colab_type": "code",
        "outputId": "2c7f7297-99ef-45c2-9690-ad20f882fdd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "#  Let's check out the redshift distribution\n",
        "%pylab inline\n",
        "plt.rc('text', usetex=False) # Override latex setting set by unagi\n",
        "hist(catalog['specz_redshift'], 100,range=[0,4]);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyWAgScJgIEnCYCBJwmAgSQL+P5t4v2RGb+nZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg7FKeNbA5Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp catalog.fits gs://{bucket_name}/hsc_photoz/data/catalog.fits catalog.fits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgRKI-Z__GAO",
        "colab_type": "text"
      },
      "source": [
        "### Download postage stamps cutouts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZr0HqPz-U7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_len = 64 # Size of cutouts in pixels\n",
        "cutout_size = 0.168*(img_len + 2.5) # Size of cutouts in Arcsecs, with some additional margin\n",
        "filters = ['HSC-G', 'HSC-R', 'HSC-I', 'HSC-Z', 'HSC-Y']\n",
        "\n",
        "tmp_dir='tmp_dir'\n",
        "out_dir='./'\n",
        "\n",
        "!mkdir -p tmp_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPkhGJqko7vR",
        "colab_type": "code",
        "outputId": "b3c8459f-204d-45cc-9751-2b4258bb1a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "cutouts_filename = task.hsc_bulk_cutout(catalog[0:1000], \n",
        "                                        cutout_size=cutout_size* u.Unit('arcsec'), \n",
        "                                        filters=filters, \n",
        "                                        archive=archive,  \n",
        "                                        nproc=2, \n",
        "                                        tmp_dir=tmp_dir, \n",
        "                                        output_dir=out_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting download of 1 batches ...\n",
            "Download filter HSC-I for batch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AstropyDeprecationWarning: tmp_dir/batch_HSC-I_0 already exists. Automatically overwriting ASCII files is deprecated. Use the argument 'overwrite=True' in the future. [astropy.io.ascii.ui]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Download finalized, aggregating cutouts.\n",
            "CPU times: user 2.17 s, sys: 556 ms, total: 2.72 s\n",
            "Wall time: 14min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16BstDA9-c7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Export the cutout file\n",
        "!gsutil cp cutouts.hdf gs://{bucket_name}/hsc_photoz/data/cutouts.hdf cutouts.hdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28r2yF7M_vyf",
        "colab_type": "text"
      },
      "source": [
        "## Store dataset in TFRecords format\n",
        "\n",
        "\n",
        "When dealing with large dataset and/or distributed computed, it quickly becomes \n",
        "complicated and inefficient to handle data...\n",
        "\n",
        "In this section, we export training and testing sets for the model stored in TFrecords format\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg5L7IJL_4jp",
        "colab_type": "code",
        "outputId": "86560521-6d1d-4ce7-c10c-547d4a2bd433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Retrieve from the cloud the catalog and cutout file\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "# If the instance is  clean, re-download data from cloud\n",
        "if not os.path.isfile('catalog.fits'):\n",
        "  !gsutil cp gs://{bucket_name}/hsc_photoz/data/catalog.fits catalog.fits\n",
        "  print('Catalog download complete')\n",
        "\n",
        "# This is the cutout file\n",
        "if not os.path.isfile('cutouts.hdf'):\n",
        "  !gsutil cp gs://{bucket_name}/hsc_photoz/data/cutouts.hdf cutouts.hdf\n",
        "  print('Cutouts download complete')\n",
        "\n",
        "# Loading the data\n",
        "catalog = Table.read('catalog.fits')\n",
        "cutouts = h5py.File('cutouts.hdf', 'r')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ahw2019/hsc_photoz/data/catalog.fits...\n",
            "| [1 files][ 68.3 MiB/ 68.3 MiB]                                                \n",
            "Operation completed over 1 objects/68.3 MiB.                                     \n",
            "Catalog download complete\n",
            "Copying gs://ahw2019/hsc_photoz/data/cutouts.hdf...\n",
            "- [1 files][ 21.2 GiB/ 21.2 GiB]   48.7 MiB/s                                   \n",
            "Operation completed over 1 objects/21.2 GiB.                                     \n",
            "Cutouts download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmxEa_avl6b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_len = 64\n",
        "\n",
        "def to_example(dictionary):\n",
        "  \"\"\"Helper: build tf.Example from (string -> int/float/str list) dictionary.\"\"\"\n",
        "  features = {}\n",
        "  for (k, v) in six.iteritems(dictionary):\n",
        "    if not v:\n",
        "      raise ValueError(\"Empty generated field: %s\" % str((k, v)))\n",
        "    if isinstance(v[0], six.integer_types):\n",
        "      features[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n",
        "    elif isinstance(v[0], float):\n",
        "      features[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n",
        "    elif isinstance(v[0], six.string_types):\n",
        "      if not six.PY2:  # Convert in python 3.\n",
        "        v = [bytes(x, \"utf-8\") for x in v]\n",
        "      features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n",
        "    elif isinstance(v[0], bytes):\n",
        "      features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n",
        "    else:\n",
        "      raise ValueError(\"Value for %s is not a recognized type; v: %s type: %s\" %\n",
        "                       (k, str(v[0]), str(type(v[0]))))\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "def _resize_image(im, size):\n",
        "  centh = im.shape[0]/2\n",
        "  centw = im.shape[1]/2\n",
        "  lh, rh = int(centh-size/2), int(centh+size/2)\n",
        "  lw, rw = int(centw-size/2), int(centw+size/2)\n",
        "  cropped = im[lh:rh, lw:rw, :]\n",
        "  assert cropped.shape[0]==size and cropped.shape[1]==size, f\"Wrong size! Still {cropped.shape}\"\n",
        "  return cropped\n",
        "\n",
        "def serialize_example(object_id):\n",
        "  \"\"\"\n",
        "  Creates a tf.Example message for corresponding object_id\n",
        "  \"\"\"\n",
        "  object_id = object_id.numpy()\n",
        "\n",
        "  # Let's extract the cutouts corresponding to this id\n",
        "  cutout = cutouts[str(object_id)]\n",
        "  im = [cutout[f]['HDU0']['DATA'][:] for f in filters]\n",
        "\n",
        "  # Reshape images to desired size\n",
        "  im_size = min([min(i.shape) for i in im])\n",
        "  im = np.stack([i[:im_size, :im_size] for i in im], axis=-1).astype('float32')\n",
        "  im = _resize_image(im, img_len)\n",
        "\n",
        "  # Serializing the images\n",
        "  serialized_output = {\"image/encoded\": [im.tostring()],\n",
        "                       \"image/format\": [\"raw\"]}\n",
        "\n",
        "  row = catalog[catalog['object_id'] == object_id]\n",
        "  \n",
        "  # Adding corresponding fields from the catalog\n",
        "  serialized_output['id'] = [int(object_id)]\n",
        "  for k in row.colnames[5:]:\n",
        "    serialized_output['attrs/'+k] = [np.asscalar(row[k])]\n",
        "\n",
        "  # Create a Features message using tf.train.Example.\n",
        "  example_proto = to_example(serialized_output)\n",
        "  return example_proto.SerializeToString()\n",
        "\n",
        "def tf_serialize_example(object_id):\n",
        "  serialized_example = tf.py_function(serialize_example, (object_id, ), tf.string)\n",
        "  return tf.reshape(serialized_example, ()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lM60zWbiaoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Split sample into training and testing set\n",
        "randomized_indices = np.random.permutation(len(catalog))\n",
        "\n",
        "# Keeping 20% of the data for testing\n",
        "n_train = int(len(randomized_indices) * 0.8)\n",
        "catalog_train = catalog[randomized_indices[:n_train]]\n",
        "catalog_test = catalog[randomized_indices[n_train:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZm0DKGKde4y",
        "colab_type": "code",
        "outputId": "b7a5b921-a9ae-40f7-ecc6-93142c09b5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# Now we serialize the data into TFRecords streams, we shard the training set\n",
        "NUM_SHARDS = 10\n",
        "\n",
        "for shard in range(NUM_SHARDS):\n",
        "  print('Writing shard %d'%shard)\n",
        "  trainset = tf.data.Dataset.from_tensor_slices(np.array(catalog_train['object_id'][shard::NUM_SHARDS]).astype('int64'))\n",
        "  serialized_trainset = trainset.map(tf_serialize_example)\n",
        "  writer = tf.data.experimental.TFRecordWriter('gs://ahw2019/hsc_photoz/tfrecords2/training-%05d-of-%05d'%(shard,NUM_SHARDS))\n",
        "  writer.write(serialized_trainset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0828 05:04:56.384631 140260974749440 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "W0828 05:04:56.420809 140260966356736 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "W0828 05:04:56.460425 140260974749440 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "W0828 05:04:56.507547 140260974749440 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing shard 0\n",
            "Writing shard 1\n",
            "Writing shard 2\n",
            "Writing shard 3\n",
            "Writing shard 4\n",
            "Writing shard 5\n",
            "Writing shard 6\n",
            "Writing shard 7\n",
            "Writing shard 8\n",
            "Writing shard 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMb7zh76K4o0",
        "colab_type": "code",
        "outputId": "56b905b9-811b-42c8-dd05-105f889c4da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "NUM_SHARDS_TEST = 1\n",
        "\n",
        "for shard in range(NUM_SHARDS_TEST):\n",
        "  print('Writing shard %d'%shard)\n",
        "  testset = tf.data.Dataset.from_tensor_slices(np.array(catalog_test['object_id'][shard::NUM_SHARDS_TEST]).astype('int64'))\n",
        "  serialized_testset = testset.map(tf_serialize_example)\n",
        "  writer = tf.data.experimental.TFRecordWriter('gs://ahw2019/hsc_photoz/tfrecords2/testing-%05d-of-%05d'%(shard,NUM_SHARDS_TEST))\n",
        "  writer.write(serialized_testset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing shard 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed27RbiVsvNK",
        "colab_type": "code",
        "outputId": "b422f070-e7a9-4995-d0e6-a6a147570e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# For good measure, let's also save the catalogs and export them to the cloud\n",
        "catalog_train.write('catalog_train.fits', overwrite=True)\n",
        "catalog_test.write('catalog_test.fits', overwrite=True)\n",
        "\n",
        "!gsutil cp catalog_*.fits gs://{bucket_name}/hsc_photoz/cat2/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://catalog_test.fits [Content-Type=application/octet-stream]...\n",
            "Copying file://catalog_train.fits [Content-Type=application/octet-stream]...\n",
            "/\n",
            "Operation completed over 2 objects/68.3 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfR43wZ_zvRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
